{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h8ERqHkQBjf"
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "One of the first probabilistic classifiers, considered as the \"hello world\" to data analysis. Is Naive because it assumes independece between the features on which we are goingo to apply the Bayes' theorem.\n",
    "\n",
    "Let us take a vector $\\textbf{x} = (x_1 , ... , x_n)$ representing n features. The probability of a certain outcome $C_k$ using Bayes' theorem is :\n",
    "\n",
    "$$ p(C_k | \\textbf{x}) = \\frac{p(C_k) p(\\textbf{x}|C_k)}{p(\\textbf{x})}$$\n",
    "\n",
    "for each $k$ in $K$.\n",
    "\n",
    "The denominator, $p(\\textrm{x})$, does not depend on $C$ and the $x_i$ values are given. So by using the joint probability and assuming that the variables are independent we obtain:\n",
    "\n",
    "$$ p(C_k|\\textbf{x}) = \\frac{ p(C_k) \\prod_{i=1}^n p(x_i|C_k) }{ \\sum_kp(C_k)p(\\textbf{x}|C_k)}$$\n",
    "\n",
    "## classifier\n",
    "\n",
    "What we have seen is the probability model. By combining the that model with a decision rule, we obtain the Naive Bayes classifier. A common rule is to pick the most probable hypothesis. In this case the function which assigns y to a clacc $C_k$ over some k is:\n",
    "\n",
    "$$y = \\underset{k\\in\\{1,...,K\\}}{\\textrm{argmax}} p(C_k)\\prod_{i=1}^n p(x_i|C_k)$$ \n",
    "\n",
    "## Example\n",
    "\n",
    "training set:\n",
    "\n",
    "| Object | mean radius | radius/ratio | roudness |\n",
    "|--------|-------------|--------------|----------|\n",
    "| track  | 5           | 1.1          | 1.3      |\n",
    "| track  | 6           | 1.5          | 1.2      |\n",
    "| track  | 12          | 1            | 1        |\n",
    "| track  | 7           | 1.2          | 1.4      |\n",
    "| track  | 8           | 1.3          | 1.9      |\n",
    "| track  | 4           | 1.7          | 1.2      |\n",
    "| track  | 9           | 2            | 2.2      |\n",
    "| track  | 5           | 1.4          | 1.5      |\n",
    "| track  | 7           | 1.3          | 1.2      |\n",
    "| dust   | 10          | 1.7          | 3        |\n",
    "| dust   | 12          | 1.8          | 1.7      |\n",
    "| dust   | 14          | 2            | 1.9      |\n",
    "| dust   | 20          | 1            | 2.5      |\n",
    "| dust   | 9           | 2.1          | 3        |\n",
    "| dust   | 8           | 1.3          | 2.6      |\n",
    "| dust   | 14          | 1.4          | 1.4      |\n",
    "| dust   | 16          | 1.6          | 1.8      |\n",
    "| dust   | 11          | 1.8          | 1.2      |\n",
    "| dust   | 13          | 2            | 1.8      |\n",
    "| dust   | 10          | 1.5          | 1.9      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUZUuw4fcYGD"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1620117506125,
     "user": {
      "displayName": "Ian Postuma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYeRJeAxy1qvpQ_t5f1YoSP7a1UZO4qbynzAQ8=s64",
      "userId": "15420211039206829163"
     },
     "user_tz": -120
    },
    "id": "rIoTj2KmQAGl",
    "outputId": "223a6c8e-4a19-417f-d419-abefa5d1af47"
   },
   "outputs": [],
   "source": [
    "# Object Dictionary that defines the first column of the data array\n",
    "ObjectDict  = {\"track\":0, \n",
    "               \"dust\":1}\n",
    "# Features Dictionary which defines the remaining columns of the data array\n",
    "FeatureDict = {\"radius\":1,\n",
    "               \"rr\":2,\n",
    "               \"roundness\":3}\n",
    "# The data containing:\n",
    "# Object type, radius, radius ratio, roundness\n",
    "train_data =   [[0,  5, 1.1, 1.3],\n",
    "                [0,  6, 1.5, 1.2],\n",
    "                [0, 12, 1,   1],\n",
    "                [0,  7, 1.2, 1.4],\n",
    "                [0,  8, 1.3, 1.9],\n",
    "                [0,  4, 1.7, 1.2],\n",
    "                [0,  9, 2,   2.2],\n",
    "                [0,  5, 1.4, 1.5],\n",
    "                [0,  7, 1.3, 1.2],\n",
    "                [0,  3, 1.0, 1.8],\n",
    "                [0,  9, 1.4, 1.2],\n",
    "                [1,  10, 1.7, 3],\n",
    "                [1,  12, 1.8, 1.7],\n",
    "                [1,  14, 2,   1.9],\n",
    "                [1,  20, 1,   2.5],\n",
    "                [1,   9, 2.1, 3],\n",
    "                [1,   8, 1.3, 2.6],\n",
    "                [1,  14, 1.4, 1.4],\n",
    "                [1,  16, 1.6, 1.8],\n",
    "                [1,  11, 1.8, 1.2],\n",
    "                [1,  13, 2,   1.8],\n",
    "                [1,  10, 1.5, 1.9]\n",
    "               ]\n",
    "train_data=np.array(train_data)\n",
    "\n",
    "# Classifier training dictionary, holding the info for the training\n",
    "# in this case we are going to use a gaussian classifier\n",
    "classifier_training = {}\n",
    "\n",
    "print(\"{:10s} {:20s} {:>10s} {:>10s}\".format(\"Class\",\"Feature\",\"Mean\",\"Var\"))\n",
    "# for each object\n",
    "for Object in ObjectDict:\n",
    "  classifier_training[Object]={}\n",
    "  # for each feature\n",
    "  for Feature in FeatureDict:\n",
    "    mean = train_data[train_data[:,0]==ObjectDict[Object]][:,FeatureDict[Feature]].mean()\n",
    "    var  = train_data[train_data[:,0]==ObjectDict[Object]][:,FeatureDict[Feature]].var()\n",
    "    # we are going to save the mean value and variance \n",
    "    # of the combined probability\n",
    "    classifier_training[Object][Feature]=[mean,var]\n",
    "    print(\"{:10s} {:20s} {:10.2f} {:10.2f}\".format(Object,Feature,mean,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJUJiG9RhUTO"
   },
   "source": [
    "## Let us classify\n",
    "\n",
    "data to classify:\n",
    "\n",
    "| Class | radius         | radius/ratio | roudness     |\n",
    "|-------|----------------|--------------|--------------|\n",
    "| track |  6.82 +- 6.15  | 1.35 +- 0.08 | 1.45 +- 0.12 |\n",
    "| dust  | 12.45 +- 10.98 | 1.65 +- 0.10 | 2.07 +- 0.34 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiBmyp69hJK-"
   },
   "outputs": [],
   "source": [
    "# Gaussian function\n",
    "def gaussian_prob(value,mean,var):\n",
    "  return np.exp(-(value-mean)**2/(2*var))/(np.sqrt(2*np.pi*var))\n",
    "\n",
    "# Prediction function\n",
    "# data_to_predict       List containing [mean radius, radius ratio, roudness]\n",
    "# weights               Containing the training dictionary\n",
    "# model                 Which is the fuction used as a model\n",
    "# ObjectDict            Object names\n",
    "# FeatureDict           Feture names\n",
    "def predict(data_to_predict , weights=classifier_training,model=gaussian_prob, ObjectDict=ObjectDict, FeatureDict=FeatureDict):\n",
    "  # array which will be used as a container for the predictions\n",
    "  predictions = np.zeros(len(ObjectDict))\n",
    "  print(data_to_predict)\n",
    "  # for each object\n",
    "  for Object in ObjectDict:\n",
    "    print(Object)\n",
    "    # for each feature\n",
    "    # the initial probability is always p(feature)\n",
    "    P = 1 / len(ObjectDict)\n",
    "    for Feature in FeatureDict:\n",
    "      # P(f|o)\n",
    "      Pfo = model(data_to_predict[FeatureDict[Feature]-1],*weights[Object][Feature])\n",
    "      # each combined probabily has to be multiplied with the object probability\n",
    "      P *= Pfo\n",
    "      print(\"     P({}|{}) = {}\".format(Feature,Object,Pfo/ len(ObjectDict)))\n",
    "    # once the loop over all features has finished we assign the probability \n",
    "    # to the predictions array, to the correct index of the object\n",
    "    predictions[ObjectDict[Object]]=P\n",
    "    print(\"P({}) = {}\".format(Object,P))\n",
    "  # now I invert the Object Dictionary to be used with the argmax function\n",
    "  inv_ObjectDict = {v: k for k, v in ObjectDict.items()}\n",
    "  return inv_ObjectDict[np.argmax(predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 723,
     "status": "ok",
     "timestamp": 1620119911470,
     "user": {
      "displayName": "Ian Postuma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYeRJeAxy1qvpQ_t5f1YoSP7a1UZO4qbynzAQ8=s64",
      "userId": "15420211039206829163"
     },
     "user_tz": -120
    },
    "id": "duOlVSpPjTf9",
    "outputId": "0ddcfded-4ec2-484b-c8be-e4b3c45f0eab"
   },
   "outputs": [],
   "source": [
    "# We have now two test values which we do not know if they are track or dust, let's ask to the model\n",
    "values = [[7,1.4,1.7],\n",
    "          [15,1.2,1.4]]\n",
    "\n",
    "for i in range(len(values)):\n",
    "  print(\"\\nRESULT:\\n{} corresponds to {} \\n\".format(values[i],predict(values[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get that the first set of values correspond to a track while the latter ones refer to dust. This was a very simple example writtes to explicitly follow each step of the calculation. The following one will be more pythonic and will use a well known datase.\n",
    "\n",
    "## Now something more serious\n",
    "\n",
    "We will use the [iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) which comes with scikit-learn.\n",
    "\n",
    "If you have to install this python library you can do it from within this notebook, by writing and executing the following line in a *code block*:\n",
    "\n",
    "```\n",
    "conda install scikit-learn -y\n",
    "```\n",
    "\n",
    "Once the block has finished the execution you can start to use the **sklearn** modules. We will begin by uploading the data and split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Import the Iris dataset\n",
    "iris = load_iris()\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is uploaded let's define a numpy function which applies the naive bayes throuhgh a gausian classifier (as the example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the classifier\n",
    "def train_naive_bayes(X, y):\n",
    "    n_samples, n_features = X.shape\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # Calculate class probabilities\n",
    "    class_probs = np.zeros(n_classes)\n",
    "    for i in range(n_classes):\n",
    "        class_probs[i] = np.mean(y == classes[i])\n",
    "    \n",
    "    # Calculate mean and variance for each feature for each class\n",
    "    means = np.zeros((n_classes, n_features))\n",
    "    variances = np.zeros((n_classes, n_features))\n",
    "    for i in range(n_classes):\n",
    "        means[i, :] = np.mean(X[y == classes[i], :], axis=0)\n",
    "        variances[i, :] = np.var(X[y == classes[i], :], axis=0)\n",
    "    \n",
    "    # Define the function to calculate the probability of a sample belonging to a class\n",
    "    def calculate_probability(sample, mean, var):\n",
    "        exponent = -((sample - mean) ** 2) / (2 * var + 1e-9)\n",
    "        return np.prod(1 / np.sqrt(2 * np.pi * var + 1e-9) * np.exp(exponent))\n",
    "    \n",
    "    # Define the function to predict the class of a sample\n",
    "    def predict(sample):\n",
    "        probabilities = np.zeros(n_classes)\n",
    "        for i in range(n_classes):\n",
    "            probabilities[i] = class_probs[i] * np.prod(calculate_probability(sample, means[i, :], variances[i, :]))\n",
    "        return classes[np.argmax(probabilities)]\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# Train the classifier using the training data\n",
    "predictor = train_naive_bayes(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = [predictor(sample) for sample in X_test]\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there are more simpler ways to use a Naive Bayes classifier, **sklearn** has the *GausianNB* class which can do this in just a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Train the classifier using the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and exercises\n",
    "\n",
    "In the code above we have seen two kind of datasets and 3 ways to apply the naive bayes classifier to the data. The aim of the notebook is to show that the training process is achieved by computing the mean and the variance of each feature, assuming that the values of those feature distribute according to a gaussian probability distribution. Even with this strong assumption, this classifier has an accuracy of just less than 0.98 on the iris dataset.\n",
    "\n",
    "**Evaluate your comprehension by answering these questions:**\n",
    "\n",
    "1. What is the size of the iris dataset ?\n",
    "2. How many features are there ?\n",
    "3. How many classes are there ?\n",
    "4. Could you produce a scatter plot of the first two features where each dot has the appropriate class color ?\n",
    "5. Why does the prediction accuracy decrease if we upload the data as follows: \n",
    "\n",
    "```\n",
    "# Split the dataset into features and labels\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "## Split the dataset into training and testing sets\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]\n",
    "```\n",
    "\n",
    "tip: try this by using `the train_naive_bayes()` function and the `sklearn` gaussian classifier `GaussianNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and labels\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "## Split the dataset into training and testing sets\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Naive Bayes classifier\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Train the classifier using the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NaiveBayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

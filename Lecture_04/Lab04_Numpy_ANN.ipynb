{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbe508",
   "metadata": {},
   "source": [
    "## Incipit: Back Propagation Algorithm\n",
    "\n",
    "Let's say we have a neural network with multiple layers, and we want to compute the gradients of the loss function with respect to the weights and biases in each layer. Here's how we can do this:\n",
    "\n",
    "1. **Forward pass**: We start by feeding the input data through the network to compute the output. During this step, we also store the intermediate values (i.e., the inputs and outputs of each layer) in memory, as we'll need them later during the backward pass.\n",
    "\n",
    "2. **Compute the loss**: Once we have the output of the network, we can compute the loss function. The loss function is typically some measure of the difference between the predicted output and the actual output.\n",
    "\n",
    "3. **Backward pass**: This is where we compute the gradients of the loss with respect to the weights and biases in each layer. We start by computing the gradient of the loss with respect to the output of the final layer (i.e., the output layer). This is done using the chain rule of calculus:\n",
    "\n",
    "```dL/dy = dL/do * do/dy```\n",
    "\n",
    "Here, `dL/dy` is the gradient of the loss with respect to the output y of the final layer, `dL/do` is the gradient of the loss with respect to the predicted output `o`, and `do/dy` is the derivative of the activation function used in the final layer.\n",
    "\n",
    "4. **Backpropagation through the layers**: Once we have the gradient of the loss with respect to the output of the final layer, we can use this to compute the gradients of the loss with respect to the inputs of the previous layer. This is done using the chain rule again:\n",
    "\n",
    "```dL/dx = dL/dy * dy/dx```\n",
    "\n",
    "Here, `dL/dx` is the gradient of the loss with respect to the input `x` of the previous layer, `dL/dy` is the gradient of the loss with respect to the output `y` of the current layer, and `dy/dx` is the derivative of the activation function used in the current layer.\n",
    "\n",
    "5. **Compute the gradients with respect to the weights and biases**: Finally, we can use the gradients computed in step 4 to compute the gradients of the loss with respect to the weights and biases in each layer. This is also done using the chain rule:\n",
    "\n",
    "```dL/dw = dL/dx * dx/dw```\n",
    "\n",
    "```dL/db = dL/dx * dx/db```\n",
    "\n",
    "\n",
    "Here, `dL/dw` is the gradient of the loss with respect to the weights `w`, `dL/db` is the gradient of the loss with respect to the bias `b`, `dL/dx` is the gradient of the loss with respect to the input `x`, and `dx/dw` and `dx/db` are the derivatives of the input `x` with respect to the weights and biases, respectively.\n",
    "\n",
    "These steps are repeated for each layer in the network, starting from the final layer and working backwards to the input layer. Once we have computed the gradients of the loss with respect to the weights and biases in each layer, we can use them to update the parameters using a gradient descent optimization algorithm.\n",
    "\n",
    "Let's consider a simple neural network with one hidden layer, one input feature, and one output. The network has the following architecture:\n",
    "\n",
    "* Input layer: 1 node\n",
    "* Hidden layer: 2 nodes, with weights `[w1, w2]` and biases `[b1, b2]`\n",
    "* Output layer: 1 node, with weight `w3` and bias `b3`\n",
    "\n",
    "The activation function is the **sigmoid function**.\n",
    "\n",
    "We will use **mean squared error** (MSE) as the loss function.\n",
    "\n",
    "Suppose we have one training example with input `x=0.5` and target output `y=0.8`. Let's initialize the weights and biases randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da163d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.5\n",
    "y = 0.8\n",
    "\n",
    "w1 = 0.2\n",
    "w2 = -0.3\n",
    "b1 = 0.4\n",
    "b2 = -0.5\n",
    "w3 = 0.1\n",
    "b3 = 0.2\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f45d3",
   "metadata": {},
   "source": [
    "To compute the forward pass, we first compute the hidden layer activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74009fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = w1 * x + b1\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "z2 = w2 * x + b2\n",
    "a2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa39f83",
   "metadata": {},
   "source": [
    "Then we compute the output layer activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b83b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "z3 = w3 * a1 + w3 * a2 + b3\n",
    "y_pred = sigmoid(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd21d3c",
   "metadata": {},
   "source": [
    "The loss function for this example is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.5 * (y - y_pred)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49518e38",
   "metadata": {},
   "source": [
    "To compute the gradient of the loss function with respect to the network parameters, we first compute the derivative of the loss function with respect to the predicted output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dy_pred = y_pred - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff9496",
   "metadata": {},
   "source": [
    "Then we can compute the gradients of the output layer weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52defbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz3_dw3 = a1 + a2\n",
    "dL_dw3 = dL_dy_pred * sigmoid_derivative(z3) * dz3_dw3\n",
    "\n",
    "dz3_db3 = 1\n",
    "dL_db3 = dL_dy_pred * sigmoid_derivative(z3) * dz3_db3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c99be",
   "metadata": {},
   "source": [
    "Next, we compute the gradients of the hidden layer activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f39afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz3_da1 = w3\n",
    "dL_da1 = dL_dw3 * dz3_da1 * sigmoid_derivative(z1)\n",
    "\n",
    "dz3_da2 = w3\n",
    "dL_da2 = dL_dw3 * dz3_da2 * sigmoid_derivative(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f85b1",
   "metadata": {},
   "source": [
    "Then, we can compute the gradients of the hidden layer weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1_dw1 = x\n",
    "dL_dw1 = dL_da1 * dz1_dw1\n",
    "\n",
    "dz1_db1 = 1\n",
    "dL_db1 = dL_da1 * dz1_db1\n",
    "\n",
    "dz2_dw2 = x\n",
    "dL_dw2 = dL_da2 * dz2_dw2\n",
    "\n",
    "dz2_db2 = 1\n",
    "dL_db2 = dL_da2 * dz2_db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999da461",
   "metadata": {},
   "source": [
    "Finally, we update the network parameters using the computed gradients and a learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "w1 -= learning_rate * dL_dw1\n",
    "b1 -= learning_rate * dL_db1\n",
    "\n",
    "w2 -= learning_rate * dL_dw2\n",
    "b2 -= learning_rate * dL_db2\n",
    "\n",
    "w3 -= learning_rate * dL_dw3\n",
    "b3 -= learning_rate * dL_db3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68fd729",
   "metadata": {},
   "source": [
    "We can repeat this process for multiple training examples to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fdfe9",
   "metadata": {},
   "source": [
    "## The effect of not having an activation function\n",
    "\n",
    "Here's an example of a neural network without an activation function that separates the same two classes of data using a vertical line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the input data and labels\n",
    "np.random.seed(0)\n",
    "X_inner = np.random.randn(100, 2) * 0.5\n",
    "X_outer = np.random.randn(100, 2) * 1.5 + np.array([0, 3])\n",
    "X = np.concatenate((X_inner, X_outer))\n",
    "y = np.concatenate((np.zeros(100), np.ones(100)))\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "lr = 0.0001\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize the weights and biases\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "# Train the neural network\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    z2 = np.dot(z1, W2) + b2\n",
    "    y_pred = z2\n",
    "\n",
    "    # Compute the error\n",
    "    error = y.reshape(-1, 1) - y_pred\n",
    "\n",
    "    # Backward pass\n",
    "    dW2 = np.dot(z1.T, error)\n",
    "    db2 = np.sum(error, axis=0, keepdims=True)\n",
    "    dW1 = np.dot(X.T, np.dot(error, W2.T))\n",
    "    db1 = np.sum(np.dot(error, W2.T), axis=0, keepdims=True)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    W1 += lr * dW1\n",
    "    b1 += lr * db1\n",
    "    W2 += lr * dW2\n",
    "    b2 += lr * db2\n",
    "\n",
    "# Evaluate the neural network on a grid of points\n",
    "xx, yy = np.meshgrid(np.linspace(-4, 4, 1000), np.linspace(-4, 7, 1000))\n",
    "X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "z1_grid = np.dot(X_grid, W1) + b1\n",
    "z2_grid = np.dot(z1_grid, W2) + b2\n",
    "y_pred = np.zeros(z2_grid.shape)\n",
    "y_pred[z2_grid<0.5]=0\n",
    "y_pred[z2_grid>=0.5]=1\n",
    "y_pred_grid = y_pred.reshape(xx.shape)\n",
    "\n",
    "# Plot the input data and the decision boundary\n",
    "plt.figure(figsize=(8, 6),dpi=300)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
    "plt.contourf(xx, yy, y_pred_grid, levels=[0, 0.5, 1], alpha=0.2, cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Neural network coordinate transformation without activation function')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d878e1",
   "metadata": {},
   "source": [
    "In this example, we define the `input` data `X` and the labels `y`. We randomly generate 100 data points inside a circle with radius 2, and 100 data points a circle of radius 3 shited by 3 along the y axis.\n",
    "\n",
    "We then define a neural network without an activation function, with a single hidden layer with 16 neurons. We initialize the weights and biases randomly and train the network using backpropagation.\n",
    "\n",
    "In the forward pass, we compute the output of the network as a linear combination of the input features, without applying an activation function. In the backward pass, we compute the gradients of the loss function with respect to the weights and biases using the chain rule.\n",
    "\n",
    "We evaluate the neural network on a grid of points and plot the result.\n",
    "\n",
    "## Applying an activation function\n",
    "\n",
    "Activation functions play a crucial role in neural networks as they introduce nonlinearity into the model, allowing it to learn complex and nonlinear relationships between the input and output data. Without activation functions, a neural network would simply be a series of linear transformations, which can only model linear relationships between the input and output data.\n",
    "\n",
    "Activation functions allow neural networks to learn complex decision boundaries and patterns in the input data. They are responsible for determining whether a neuron in the network should be activated or not, based on its input. This activation or non-activation of a neuron is then propagated through the network, allowing it to learn more complex representations of the input data.\n",
    "\n",
    "Different activation functions have different properties, and choosing the right activation function for a given task can have a significant impact on the performance of a neural network. For example, the sigmoid function is commonly used for binary classification tasks, while the ReLU function is widely used in deep learning models due to its ability to accelerate training and reduce the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23235791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the input data and labels\n",
    "np.random.seed(0)\n",
    "X_inner = np.random.randn(100, 2) * 0.5\n",
    "X_outer = np.random.randn(100, 2) * 1.5 + np.array([0, 3])\n",
    "X = np.concatenate((X_inner, X_outer))\n",
    "y = np.concatenate((np.zeros(100), np.ones(100)))\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "lr = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Initialize the weights and biases\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "# Train the neural network\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    y_pred = sigmoid(z2)\n",
    "\n",
    "    # Compute the error\n",
    "    error = y.reshape(-1, 1) - y_pred\n",
    "\n",
    "    # Backward pass\n",
    "    delta2 = error * sigmoid_derivative(z2)\n",
    "    dW2 = np.dot(a1.T, delta2)\n",
    "    db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "    delta1 = np.dot(delta2, W2.T) * sigmoid_derivative(z1)\n",
    "    dW1 = np.dot(X.T, delta1)\n",
    "    db1 = np.sum(delta1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    W1 += lr * dW1\n",
    "    b1 += lr * db1\n",
    "    W2 += lr * dW2\n",
    "    b2 += lr * db2\n",
    "\n",
    "# Evaluate the neural network on a grid of points\n",
    "xx, yy = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 7, 100))\n",
    "X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "z1_grid = np.dot(X_grid, W1) + b1\n",
    "a1_grid = sigmoid(z1_grid)\n",
    "z2_grid = np.dot(a1_grid, W2) + b2\n",
    "y_pred_grid = sigmoid(z2_grid)\n",
    "y_pred_grid = y_pred_grid.reshape(xx.shape)\n",
    "\n",
    "# Plot the input data and the decision boundary\n",
    "plt.figure(figsize=(8, 6),dpi=300)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
    "plt.contourf(xx, yy, y_pred_grid, levels=[0, 0.5, 1], alpha=0.2, cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Neural network coordinate transformation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735b999",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. What happens when you change the number of epochs (try to decrease the epochs to 10)\n",
    "2. What happens when you change the learning rate ?\n",
    "3. What happens if tou change the hidden layer dimension ?\n",
    "\n",
    "### Advanced exercise\n",
    "\n",
    "1. Divide the training process in batches\n",
    "2. register the training history (i.e. compute and store the loss at the end of each epoch)\n",
    "3. how does the loss change when changing the batch size ?\n",
    "\n",
    "## Appendix\n",
    "\n",
    "The vanishing gradient problem is a common issue in deep neural networks that can occur during the training process. It refers to the phenomenon where the gradients (i.e., the derivatives of the loss function with respect to the model parameters) become very small as they propagate backward through the network, towards the earlier layers.\n",
    "\n",
    "This can happen because of the way the gradients are computed and propagated through the layers of the network. In particular, when the network has many layers, the gradients can get smaller and smaller as they are multiplied by the weights in each layer, which can lead to very small or negligible updates to the earlier layers in the network.\n",
    "\n",
    "The vanishing gradient problem can make it difficult for the network to learn meaningful representations of the input data, especially for deeper networks. It can also make the training process slower and less stable, as the network is not able to make meaningful updates to its parameters.\n",
    "\n",
    "To address the vanishing gradient problem, several activation functions have been proposed that are more robust to this issue, such as the rectified linear unit (ReLU) and its variants. Additionally, other techniques such as weight initialization, batch normalization, and skip connections have also been proposed to alleviate the vanishing gradient problem and improve the training of deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db131c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
